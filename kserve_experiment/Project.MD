## ğŸ“‚ í´ë” êµ¬ì¡°

```plaintext
llm-kserve-experiment/
â”œâ”€â”€ app.py                 # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ inference.yaml         # KServe InferenceService ì„¤ì • íŒŒì¼
â”œâ”€â”€ send_request.py        # í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ìš© ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt       # Python ì˜ì¡´ì„± ëª©ë¡
â”œâ”€â”€ Dockerfile             # ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œë¥¼ ìœ„í•œ ë„ì»¤íŒŒì¼
â””â”€â”€ README.md              # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
```

## ğŸš€ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” Hugging Faceì˜ `distilgpt2` ëª¨ë¸ì„ KServeë¥¼ í†µí•´ ì„œë²„ë¦¬ìŠ¤ ë°©ì‹ìœ¼ë¡œ ë°°í¬í•˜ê³ , ë¡œì»¬ì—ì„œ port-forwardë¥¼ ì‚¬ìš©í•´ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

## ğŸ”§ ì‚¬ì „ ì¤€ë¹„

1. **Kubernetes í´ëŸ¬ìŠ¤í„°** (ì˜ˆ: Kind, Minikube) ì„¤ì¹˜ ë° ì¤€ë¹„
2. **KServe** ì„¤ì¹˜ ë° ì„¤ì • ì™„ë£Œ
3. **kubectl** ëª…ë ¹ì–´ ì‚¬ìš© ê°€ëŠ¥
4. ë¡œì»¬ì— **Python 3.8+** ì„¤ì¹˜
5. (ì„ íƒ) Docker ë° ì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì ‘ê·¼ ê¶Œí•œ

## âš™ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰

1. ì˜ì¡´ì„± ì„¤ì¹˜

   ```bash
   cd llm-kserve-experiment
   pip install -r requirements.txt
   ```

2. ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œ & í‘¸ì‹œ

   ```bash
   docker build -t <your-registry>/llm-server:latest .
   docker push <your-registry>/llm-server:latest
   ```

3. `inference.yaml` ì— ì´ë¯¸ì§€ ê²½ë¡œ ì—…ë°ì´íŠ¸

   ```yaml
   spec:
     predictor:
       containers:
       - image: <your-registry>/llm-server:latest
         name: llm-container
   ```

4. InferenceService ìƒì„±

   ```bash
   kubectl apply -f inference.yaml -n kserve
   ```

5. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸

   ```bash
   kubectl get inferenceservice llm-server -n kserve
   ```

6. Port-forward ì‹¤í–‰ (ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ)

   ```bash
   kubectl port-forward svc/llm-server-predictor-00001 8000:80 -n kserve
   ```

7. ë¡œì»¬ í…ŒìŠ¤íŠ¸

   * **curl**

     ```bash
     curl -X POST http://localhost:8000/predict \
       -H "Content-Type: application/json" \
       -d '{"prompt":"Hello, world!"}'
     ```
   * **send\_request.py**

     ```bash
     python send_request.py
     ```

## ğŸ“„ íŒŒì¼ ì„¤ëª…

* **app.py**: FastAPI ê¸°ë°˜ ì„œë²„ ì½”ë“œ. ëª¨ë¸ ë¡œë“œ ë° `/predict` ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„.
* **inference.yaml**: KServe InferenceService CRD. ëª¨ë¸ ì´ë¯¸ì§€, ë¦¬ì†ŒìŠ¤ ì„¤ì • í¬í•¨.
* **send\_request.py**: `requests` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ ë¡œì»¬ ì—”ë“œí¬ì¸íŠ¸ì— í…ŒìŠ¤íŠ¸ ìš”ì²­ì„ ë³´ë‚´ê³  ê²°ê³¼ ì¶œë ¥.
* **requirements.txt**: `fastapi`, `uvicorn`, `transformers`, `requests` ë“± í•„ìš” íŒ¨í‚¤ì§€ ëª©ë¡.
* **Dockerfile**: FastAPI ì•±ì„ ì»¨í…Œì´ë„ˆí™”í•˜ê¸° ìœ„í•œ ì„¤ì •.

## ğŸ› ï¸ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

* **Port-forward ì—°ê²° ì‹¤íŒ¨**: ì„œë¹„ìŠ¤ ì´ë¦„(`svc/llm-server-predictor-00001`)ê³¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤(`kserve`) í™•ì¸
* **Read timed out**: í´ë¼ì´ì–¸íŠ¸ `timeout` ê°’ ì¡°ì • ë˜ëŠ” ëª¨ë¸ ë¡œë”© ì‹œê°„ í™•ì¸
* **Pod ë¯¸ìƒì„±**: `kubectl describe inferenceservice llm-server -n kserve` ë¡œ ì´ë²¤íŠ¸ í™•ì¸

## ğŸ“š ì°¸ê³ 

* [KServe ê³µì‹ ë¬¸ì„œ](https://kserve.github.io/)